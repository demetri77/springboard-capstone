---
title: "Banking Capstone - Machine Learning Applied"
author: "Demetri Lee"
date: "December 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

## Trying to predict the success of bank telemarketing

I'm trying to predict the success of telemarketing calls for a bank that's 
selling long-term deposits. A Portuguese retail bank was addressed, with data 
collected from 2008 to 2013, thus including the effects of the recent 
financial crisis of 2008. With 21 features available for analysis, I hope to 
derive knowledge for a model that would confirm how to better focus resources 
towards clients with a high chance of agreeing to registering for extra 
banking services. 

```{r}
bankfull_clean <- read_csv("bankfull_clean.csv", col_types = "ifffffffffiiiifdddddf")
# "iccccccccciiiicdddddc"

```

I will be treating this a supervised classification problem. While predicting 
whether a customer will subscribe to a new service, I need to find the main 
predictors while developing a model that best expressses a client's decision 
for subscribing. 

## Set Up Model Parameters

Let's start with partitioning the dataset into a training, validation, and 
test set.

```{r partition}
set.seed(13456)
# Split the full dataset into 80% and 20% partitions
TrainingValidationIndex <- createDataPartition(y=bankfull_clean$subscribed, p=0.80, list=FALSE)
training_validation <- bankfull_clean[TrainingValidationIndex,]

# Split the main 80% into a Training (75%) and Validation (25%) set
trainIndex <- createDataPartition(training_validation$subscribed, p=0.75, list=FALSE)
training <- training_validation[trainIndex,]
validation <- training_validation[-trainIndex,]

# Original 20% kept aside as Test Data
testData  <- bankfull_clean[-TrainingValidationIndex,]

#trControl(sampling)
control <- trainControl(method = "cv", number = 2, classProbs = TRUE, verboseIter = TRUE)
```

## Logistic Regression with CARET

Develop a model with the training data. 

```{r logReg, warning=FALSE}
# develop a model
set.seed(7)
glm_model_full <- train(subscribed ~ . - duration_sec, data=training, method="glm", na.action=na.omit, metric="Kappa")
```


What can we conclude from the logistic regression model? 

```{r}
summary(glm_model_full) # results with entire variable set minus the duration of last call
```

Let's apply our model to the validation set.

```{r}
set.seed(7)
glm_predict_full <- predict(glm_model_full, newdata=validation)
```


Here is a confusion matrix, with the results expressing accuracy, sensitivity, 
and specificity.

```{r}
caret::confusionMatrix(glm_predict_full, validation$subscribed, mode="everything", positive="yes") 

# use metric parameter Kappa because it's better than F1 for uneven results.

# investigate further parameters (eg F1/accuracy score)
# Precision = TP / (TP + FP)
# Recall = TP / (TP + FN)
```

**Comparing results of Logistic Regression model:**

* Kappa = 0.2742
* F1 = 0.32308

## Random Forest with CARET

Develop a model with training data.

```{r rdmForest}
# play with smaller # of rf while playing/testing
set.seed(7)
rf_model <- caret::train(subscribed~.-duration_sec, data=training, method="rf", metric="Kappa", 
                         trControl=trainControl(method="cv", number=10))

summary(rf_model)
```


Apply the Random Forest model to the Validation set.

```{r}
set.seed(7)
rf_prediction <- predict(rf_model, newdata=validation)
```


Review the Confusion Matrix for the Random Forest model.

```{r}
caret::confusionMatrix(rf_prediction, validation$subscribed, mode="everything", positive="yes")
```

**Comparing results of Random Forest:**

10 FOLDS:

* Kappa = 0.5145
* F1 = 0.9481

10 FOLDS (updated):

* Kappa = 0.3182
* F1 = 0.37143

Because the Kappa value for the Random Forest model is higher than the Logistic Regression model, let's choose the Random Forest model for the final prediction with the test data.

```{r}
set.seed(7)
predict_testData <- predict(rf_model, newdata=testData)
caret::confusionMatrix(predict_testData, testData$subscribed, mode="everything", positive="yes")
```

**Confirming against Test Data**

* Kappa = 0.4039
* F1 = 0.44928

The Kappa score is higher with the test data than the training or validation 
set. With the test data, the positive predictive value claims this model will 
be correct 64.583% of the time and incorrect 35.417% of the time.